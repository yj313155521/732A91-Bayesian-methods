---
title: "Bayesian Learning Lab 3"
author: 
- "Shipeng Liu"
- "Jin Yan"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
library(mvtnorm)
library(ggplot2)
library(reshape2)
library(StanHeaders)
library(rstan)
```

\newpage

# Assignment 1

## a) Evaluate the convergence of the Gibbs sampler
```{r}
#1.1
data <- readRDS("Precipitation.rds")
# set initial parameters
n <- length(data)
mu_0 <- 1.5
tao_0 <- 1000
nu_0 <- 1
sigma_0_sq <- 1
nu_n <- n+nu_0
mean_lny <- mean(log(data))
sigma_sq_vector <- c(10000)
mu_vector <- NULL
# the following is used to get 1000 samples for mu and sigma.
for(i in 1:1000){
  sigma_sq <- sigma_sq_vector[i]
  #The following is find mu_n and tao_n
  tao_n <- sqrt(sigma_sq*tao_0^2/(n*tao_0^2+sigma_sq))
  w <- n/sigma_sq /(n/sigma_sq+ 1/tao_0^2)
  mu_n <- w*mean_lny+(1-w)*mu_0

  # the following is used to find mu
  mu <- rnorm(mu_n,tao_n)
  mu_vector[i+1] <- mu
  
  # the following is used to find new sigma_sq
  X <- rchisq(1,nu_n)
  s_sq <- nu_0*sigma_0_sq + sum((log(data)-mu)^2)/(n+nu_0)
  sigma_sq <- nu_n*s_sq/X
  sigma_sq_vector[i+1] <- sigma_sq
}


a_Gibbs <- acf(mu_vector[-1]) #not sure if it is right to omit the first element
IF_Gibbs <- 1+2*sum(a_Gibbs$acf[-1])
cat("IF_Gibbs =",IF_Gibbs)
# plot trajectories of the samples.
plot(1:1000,mu_vector[-1],col="blue",main = "trajectories for mu samples",type = "l")
```

From above the value of IF_Gibbs and plot we can see that the mu samples generated by Gibbs sampler converge pretty well.

## b) How well does the posterior predictive density agree with this data?
```{r}
#1.2
# find the matched mu and sigma square
mu <- mu_vector[1001]
sigma <- sqrt(sigma_sq_vector[1001])

hist(log(data),freq = FALSE,main = "Daily Precipitation and Predictive Density")
hist_obj <- hist(log(data),plot = FALSE)
density_values <- dnorm(hist_obj$mids,mean = mu,sd=sigma)
#add density curve
lines(hist_obj$mid,density_values,col="blue",lwd=2)
```


From the histogram we can see that in general the tendency is pretty similar,
but not exact.

# Assignment 2
```{r}
ebaydata <- read.table("eBayNumberOfBidderData.dat", header = TRUE)
```

## a) Metropolis Random Walk for Poisson regression

### Obtain the maximum likelihood estimator of $\beta$ in the Poisson regression model for the eBay data

```{r}
# Obtain the model
model=glm(nBids~.,family = 'poisson',data=ebaydata[,-2])

summary(model)
```

It seem's the covirate MinBidShare is the most significant,Negatively correlated with outcome.The larger the ratio the minimum selling price to the book value,the smaller the number of bids.This conclusion is intuitive and reasonable.

## b) $\widetilde{\beta}$ and $J_y(\widetilde{\beta})$

```{r}
# Parameters
X=as.matrix(ebaydata[,-1])
y=ebaydata[,1]
mu=as.matrix(rep(0,dim(X)[2]))
sigma=as.matrix(100*solve(t(X)%*%X))

initBeta=as.matrix(rep(0,dim(X)[2]))

# The function to compute the posterior distribution
logPosterior=function(beta,mu,sigma,X,y){
  # The dimension of beta is 1*9(Including Intercept)
  
  # Prior density is multinorm distribution
  logPrior=dmvnorm(c(beta),mean=c(mu),sigma=sigma,log=TRUE)
  
  # Log Likelihood
  logLik=sum(y*(X%*%beta))-sum(exp(X%*%beta))#-sum(factorial(y))
  # The factorial can be omit
  
  logPoster=logLik+logPrior
  
  return(logPoster)
  }

OptimRes=optim(initBeta,logPosterior,gr=NULL,
               mu,sigma,X,y,method=c('BFGS'),
               control=list(fnscale=-1),hessian=TRUE)

# Obtain the mode of beta and it's negative Hessian matrix
betaMode=OptimRes[["par"]]
betaHessian=-OptimRes$hessian
Jbetamode=solve(betaHessian)

cat('The posterior mode is:\n')
print(betaMode)
cat('\nThe posterior covariance is:\n')
print(Jbetamode)
```

The mode of posterior beta is close to what we get using the glm().

## c) Simulate using Metropolis Algorithm

```{r}
# A general function that uses the Metropolis algorithm to generate random draw from arbitrary posterior density
RWMSampler=function(num,logPostFunc,c){
  sampleData=data.frame(matrix(0,nrow=num,ncol=9))
  colnames(sampleData)=colnames(X)
  # The first parameter should be theta
  theta=rmvnorm(1,betaMode,c*Jbetamode)
  sampleData[1,]=theta
  
  count=1
  while(count<num){
    theta=as.numeric(sampleData[count,])
    thetap=as.numeric(rmvnorm(1,theta,c*Jbetamode))
    acceptance_rate=min(1,
          exp(logPostFunc(thetap,mu,sigma,X,y)-logPostFunc(theta,mu,sigma,X,y)))
    if(runif(1,0,1)<acceptance_rate){
      count=count+1
      sampleData[count,]=thetap
    }
  }
  return(sampleData)
}

#Sample from the posterior of beta
sampleData=RWMSampler(1000,logPosterior,2)

print(sampleData[1:20,])
```

### Assess MCMC convergence by graphical methods

```{r}
sampleData_melted <- melt(sampleData)
ggplot(sampleData_melted, aes(x = value, fill = variable)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ variable, scales = "free") +
  labs( x = "Value", y = "Density",title='Density Histogram of covariants
',tag='Fig 2.3.1')

sampleData_melted['index']=rep(seq(1,1000,1),9)
ggplot(sampleData_melted, aes(x = index,y=value, fill = variable)) +
  geom_line(alpha = 0.5) +
  facet_wrap(~ variable, scales = "free") +
  labs( x = "Iteration", y = "Value",title='Value of covariants
',tag='Fig 2.3.2')
```

It can be seen that they all converge to a certain normal distribution.

## d) Simulate from the predictive distribution of the number of bidders in a new auction 

```{r}
# 1000 Sample from the RWMSampler
# PowerSeller=1,VerifyID=1,Sealed=1,MinBlem=0,MajBlem=1
# LargNeg=0,LogBook=1.2,MinBidShare=0.8
params=c(1,1,1,1,0,1,0,1.2,0.8)
result=data.frame(matrix(0,nrow=nrow(sampleData),ncol=0))
for(i in 1:nrow(sampleData)){
  lambda=exp(params%*%as.numeric(sampleData[i,]))
  result[i,1]=rpois(1,lambda)
}

colnames(result)=c('y')

#Plot
ggplot(data=result,aes(x=y))+
  geom_histogram(aes(y=..density..),binwidth=1,alpha=0.5)+
  geom_density(color='red',size=0.7,adjust=2.5)+
  labs(title='The predictive distribution of the number of bidders
',tag='Fig 2.4')+
  xlab("the number of bidders (y)")+
  theme_bw()
```

```{r}
cat('The probability of no bidders in this new auction:',mean(result==0))

```

# Assignment 3

## a) Write a function in R that simulates data from the AR(1)-process

```{r}
# Write a function in R that simulates data from the AR(1)-process
simulateAR=function(T_num,mu,phi,sigma){
  x=mu
  sampleData=c()
  for(i in 1:T_num){
    x=mu+phi*(x-mu)+rnorm(1,0,sqrt(sigma))
    sampleData=c(sampleData,x)
  }
  return(sampleData)
}

mu=13
sigma=3
T_num=300

# Simulations
# Phi=-0.3
sampleData1=simulateAR(T_num,mu,-0.3,sigma)
ggplot(data.frame("x_t"=sampleData1,"T"=1:T_num))+geom_line(aes(x=T,y=x_t))+
  labs(title='Simulated data from the AR(1)-process
',subtitle="Phi=-0.3",tag='Fig 3.1.1')+
  xlab("T")+
  ylab("x_t value")+
  theme_bw()

# Phi=0
sampleData2=simulateAR(T_num,mu,0,sigma)
ggplot(data.frame("x_t"=sampleData2,"T"=1:T_num))+geom_line(aes(x=T,y=x_t))+
  labs(title='Simulated data from the AR(1)-process
',subtitle="Phi=0",tag='Fig 3.1.2')+
  xlab("T")+
  ylab("x_t value")+
  theme_bw()

# Phi=0.6
sampleData3=simulateAR(T_num,mu,0.6,sigma)
ggplot(data.frame("x_t"=sampleData3,"T"=1:T_num))+geom_line(aes(x=T,y=x_t))+
  labs(title='Simulated data from the AR(1)-process
',subtitle="Phi=0.6",tag='Fig 3.1.3')+
  xlab("T")+
  ylab("x_t value")+
  theme_bw()

```

### What effect does the value of Phi have on x1:T ?

When the value of Phi is positive, it means that there is a positive correlation between the current observation and the past observation. Therefore, the simulated numerical series may have a continuously increasing or continuously decreasing trend.

When the value of Phi is negative, it means that there is a negative correlation between the current observation and the past observation. Therefore, the simulated numerical series may have oscillatory or periodic characteristics.

In addition, the value of Phi will also affect the stability of the simulated sequence. When the absolute value of Phi is less than 1, the simulated numerical sequence is usually stable. And when the absolute value of Phi is greater than 1, the simulated numerical sequence may unable to reach a stable state.

## b) Simulate two AR(1)-processes

```{r}
mu=13
sigma=3
T_num=300

# Phi=0.2
sampleData4=simulateAR(T_num,mu,0.2,sigma)

# Phi=0.95
sampleData5=simulateAR(T_num,mu,0.95,sigma)

```

### Building the stan model

```{R}
stancode='
// Save as armodel.stan
data {
  int<lower=0> J;         //The number of observations
  vector[J] y;              //The observations
}

parameters {              //Parameters
  real mu;               
  real phi;
  real<lower=0> sigma;
}

model {
  // prior distribution(Non-informative)
  mu ~ normal(0,100);
  phi ~ uniform(-1,1);
  sigma ~ scaled_inv_chi_square(1,2);
  
  for (n in 2:J)
    y[n] ~ normal(mu+phi*(y[n-1]-mu),sqrt(sigma));
  
}'

```

### i) Report the posterior mean, 95% credible intervals and the number of effective posterior samples for the three inferred parameters for each of the simulated AR(1)-process.

```{r,warning=FALSE}

# Phi=0.2
fit1 <- stan(model_code =stancode, data =list(J=300,y=sampleData4), iter = 2000, warmup = 1000, chains = 4)

posterior_mean_1 <- summary(fit1)$summary[1:3, c(1,4,8)]
cat("Phi=0.2:\nPosterior Mean:\n")
print(posterior_mean_1[,1])
cat("95% credible intervals:\n")
print(posterior_mean_1[,c(2,3)])

# Effective posterior samples
params_1=extract(fit1)
params_1_data=data.frame("iteration"=1:4000,"mu"=params_1$mu,"phi"=params_1$phi,"sigma"=params_1$sigma)

#Plot
# mu
ggplot(data=params_1_data)+geom_line(aes(x=iteration,y=mu))+
  labs(title='Posterior mu from the AR(1)-processs sample
',subtitle="true Phi=0.2,true mu=13",tag='Fig 4.1.1')+
  xlab("Iteration")+
  ylab("mu value")+
  theme_bw()

# phi
ggplot(data=params_1_data)+geom_line(aes(x=iteration,y=phi))+
  labs(title='Posterior phi from the AR(1)-process sample
',subtitle="true Phi=0.2",tag='Fig 4.1.2')+
  xlab("Iteration")+
  ylab("phi value")+
  theme_bw()

#sigma
ggplot(data=params_1_data)+geom_line(aes(x=iteration,y=sigma))+
  labs(title='Posterior mu from the AR(1)-processs sample
',subtitle="true Phi=0.2,true sigma=3",tag='Fig 4.1.3')+
  xlab("Iteration")+
  ylab("sigma value")+
  theme_bw()
```

We obtained 1000 samples of four Markov chains after 1000 warmups, for a total of 4000. From the figure, mu, phi, and sigma all converge to the real value, and only a few samples are far from the real value. So the number of effective posterior samples is close to 4000.


```{r}
# Phi=0.95
fit2 <- stan(model_code =stancode, data = list(J=300,y=sampleData5), iter = 2000, warmup = 1000, chains = 4)

posterior_mean_2 <- summary(fit2)$summary[1:3, c(1,4,8)]
cat("Phi=0.95:\nPosterior Mean:\n")
print(posterior_mean_2[,1])
cat("95% credible intervals:\n")
print(posterior_mean_2[,c(2,3)])

# Effective posterior samples
params_2=extract(fit2)
params_2_data=data.frame("iteration"=1:4000,"mu"=params_2$mu,"phi"=params_2$phi,"sigma"=params_2$sigma)

#Plot
# mu
ggplot(data=params_2_data)+geom_line(aes(x=iteration,y=mu))+
  labs(title='Posterior mu from the AR(1)-processs sample
',subtitle="true Phi=0.95,true mu=13",tag='Fig 4.1.4')+
  xlab("Iteration")+
  ylab("mu value")+
  theme_bw()

# phi
ggplot(data=params_2_data)+geom_line(aes(x=iteration,y=phi))+
  labs(title='Posterior phi from the AR(1)-process sample
',subtitle="true Phi=0.95",tag='Fig 4.1.5')+
  xlab("Iteration")+
  ylab("phi value")+
  theme_bw()

#sigma
ggplot(data=params_2_data)+geom_line(aes(x=iteration,y=sigma))+
  labs(title='Posterior mu from the AR(1)-processs sample
',subtitle="true Phi=0.95,true sigma=3",tag='Fig 4.1.6')+
  xlab("Iteration")+
  ylab("sigma value")+
  theme_bw()

```

The posterior sample of phi and sigma value is close to the real value,but the mu value haven't converge and many sample a very different from the real value.So the number of effective posterior samples is much smaller than 4000.

### ii) For each of the two data sets, evaluate the convergence of the samplers and plot the joint posterior of mu and phi. Comments?

```{r}
# Plot the joint posterior
# phi=0.2
ggplot(data=params_1_data)+geom_point(aes(x=mu,y=phi),color='orange')+
  labs(title='Joint posterior of mu and phi from the sampler
',subtitle="true Phi=0.2,true mu=13",tag='Fig 4.2.1')+
  xlab("mu")+
  ylab("phi")+
  theme_bw()

# phi=0.95
ggplot(data=params_2_data)+geom_point(aes(x=mu,y=phi),color='lightgreen')+
  labs(title='Joint posterior of mu and phi from the sampler
',subtitle="true Phi=0.95,true mu=13",tag='Fig 4.2.2')+
  xlab("mu")+
  ylab("phi")+
  theme_bw()

```

It can be seen that the sampler at phi=0.2 converges well, but the sampler at phi=0.95 does not converge, and the larger the phi, the larger the variance of mu.

The reason may be,as in the 3 a) said,when the value of Phi is positive,the simulated numerical series may have a continuously increasing or continuously decreasing trend.In addition, the value of Phi will also affect the stability of the simulated sequence.

\newpage

# Appendix

```{r, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```


